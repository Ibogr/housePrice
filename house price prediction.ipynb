# Importing necessary libraries
import pandas as pd  # For data manipulation and analysis
import numpy as np  # For numerical computations
import seaborn as sns  # For data visualization
import matplotlib.pyplot as plt  # For plotting graphs
import warnings  # To handle warning messages

warnings.simplefilter(action="ignore")  # Suppresses warnings to keep the output clean

# Reading data from a CSV file
data_ = pd.read_csv('train.csv')  # Loads the dataset into a DataFrame
data_

data_.shape  # Prints the number of rows and columns in the dataset

# Making a copy of the data to work with
df = data_.copy()
df.columns  # Displays all column names in the dataset

# Checking for missing values in the first 20 columns
df.isnull().sum().head(20)

# Identifying data types in the dataset
obj = []  # List to store columns with object (categorical) type
int64 = []  # List to store columns with integer type
float64 = []  # List to store columns with float type

for i in df.columns:  # Loop through all columns
    if df[i].dtype == 'O':  # Check if the column is of object type
        obj.append(i)
    elif df[i].dtype == "float64":  # Check if the column is of float type
        float64.append(i)
    else:  # Otherwise, it's an integer type
        int64.append(i)

# Printing the columns grouped by their data types
print("<**type object**>:    ", obj)
print("-*-" * 36)
print("<**type int**>:    ", int64)
print("-*-" * 36)
print("<**type float**>:    ", float64)

# Visualizing the number of unique values in categorical columns
unique_values = []
for col in obj:  # Loop through all categorical columns
    unique_values.append(df[col].unique().size)  # Count unique values

plt.figure(figsize=(10, 6))  # Set figure size
plt.title('No. Unique values of Categorical Features')  # Title of the plot
plt.xticks(rotation=80)  # Rotate x-axis labels for better visibility
sns.barplot(x=obj, y=unique_values);  # Create a bar plot

# Replacing codes in the MSZoning column with more descriptive labels
df.MSZoning = df.MSZoning.replace({
    'RL': 'Residential Low Density',
    'A': 'Agriculture',
    'C(all)': 'Commercial',
    'FV': 'Floating Village Residential',
    'I': 'Industrial',
    'RH': 'Residential High Density',
    'RP': 'Residential Low Density Park',
    'RM': 'Residential Medium Density'
})

df.MSZoning.value_counts()  # Counts the frequency of each value in the MSZoning column

# Visualizing the distribution of the MSZoning column
plt.figure(figsize=(10, 5))  # Set figure size
sns.countplot(df['MSZoning'])  # Count plot for MSZoning
plt.xticks(rotation=80);  # Rotate x-axis labels

# Selecting specific rows where MSZoning is "Residential Low Density"
specific_data = df[df['MSZoning'] == 'Residential Low Density']
specific_data

# Grouping data by MSZoning and calculating summary statistics
df.groupby(df['MSZoning'] == 'Residential Low Density').agg({'SalePrice': ['count', 'min', 'max', 'std', 'mean']}).round()

# Identifying columns with more than 200 missing values
nullvalues = []
for null in df:
    if df[null].isnull().sum() > 200:  # Check if null values exceed 200
        nullvalues.append(null)

nullvalues  # List of columns with over 200 missing values

# Dropping columns with too many missing values
df.drop(['LotFrontage', 'Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1, inplace=True)

# Filling missing values using different methods
df['BsmtQual'].fillna((df['BsmtQual'].ffill), inplace=True)  # Forward filling for BsmtQual
df["MasVnrArea"].fillna((df['MasVnrArea'].mean()), inplace=True)  # Filling with mean for MasVnrArea
df['BsmtCond'].fillna(df['BsmtCond'].ffill(), inplace=True)  # Forward filling for BsmtCond
df['GarageFinish'].fillna(df['GarageFinish'].ffill(), inplace=True)  # Forward filling for GarageFinish
df.dropna(inplace=True)  # Dropping remaining rows with missing values

df.describe(percentiles=[0.01, 0.99, 0.25, 0.75, 0.5]).drop('count').round(2).T  # Statistical summary of the dataset

# Selecting specific columns for the model
modelColumns = {'YrSold', 'MoSold', 'OpenPorchSF', 'GarageCars', 'LotArea', 'YearRemodAdd', 'OverallQual', 'GrLivArea', 'FullBath', 'YearBuilt', '1stFlrSF', 'SalePrice'}
data = pd.DataFrame(data=df, columns=modelColumns)  # Creating a new DataFrame with selected columns

# Adding a new column to classify months into seasons
data['Seasons'] = ['winter' if i in [12, 1, 2] else
                   'spring' if i in [3, 4, 5] else
                   'summer' if i in [6, 7, 8] else
                   'autumn' if i in [9, 10, 11] else ''
                   for i in data['MoSold']]

# Adding a new column for garden area
data['Garden'] = data['LotArea'] - data['1stFlrSF']  # Calculating garden area by subtracting building size

data.shape  # Printing the shape of the modified DataFrame

# Displaying column names and data types
data.columns
data.info()

# Importing necessary libraries for machine learning
from sklearn import model_selection  # For splitting the data into training and testing sets
from sklearn.linear_model import LinearRegression  # For linear regression modeling
from sklearn.ensemble import RandomForestRegressor  # For random forest regression
from sklearn.preprocessing import StandardScaler  # For feature scaling
from sklearn.metrics import accuracy_score, recall_score  # For evaluating model performance
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score  # For model tuning and splitting data
from sklearn.metrics import mean_squared_error  # For calculating model errors
from sklearn.cluster import KMeans  # For clustering data
from xgboost import XGBRegressor  # For gradient boosting regression using XGBoost
from sklearn.ensemble import GradientBoostingRegressor  # For gradient boosting regression

# Converting categorical variables into dummy/indicator variables
df = pd.get_dummies(df, drop_first=True)  # Drop the first category to avoid multicollinearity

# Defining independent variables (X) and target variable (y)
X = df.drop(['SalePrice'], axis=1)  # Features: all columns except 'SalePrice'
y = df['SalePrice']  # Target: 'SalePrice'

# Splitting the dataset into training and testing subsets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=100)  # 70% training, 30% testing

# Building a Linear Regression model
model = LinearRegression()  # Initializing the Linear Regression model
model.fit(X_train, y_train)  # Fitting the model to the training data
y_predict = model.predict(X_test)  # Predicting target values for the test set

# Evaluating the model's performance
train_score = model.score(X_train, y_train)  # R² score for training set
test_score = model.score(X_test, y_test)  # R² score for testing set
print('Train R²:', train_score)  # Printing R² for the training set
print('Test R²:', test_score)  # Printing R² for the testing set

# Calculating the Root Mean Squared Error (RMSE)
housing_predictions = model.predict(X_test)  # Predictions for the test set
error = mean_squared_error(y_test, housing_predictions)  # Mean squared error
rmse = np.sqrt(error)  # Root mean squared error
print("RMSE:", rmse)
print("Normalized RMSE:", rmse / y_test.mean())  # RMSE as a percentage of the mean target value
